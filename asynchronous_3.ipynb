{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfHI5eZehjO1jvlKiDnslN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 非同期関数を用いて指定したurlからデータを取得する\n",
        "非同期処理の学習のため、実際に動くコードを書きながら理解を深めました。\"指定したurlからデータを取る\"という切り口で整理してみましたので、ご参考まで。\n",
        "\n",
        "### 指定したurlからデータを取得する方法\n",
        "同期的処理でデータを取得する方法は複数あります。有名なものは以下の通りでしょうか（本記事の下方に実際にQiita, Zennへアクセスしてデータを取得する処理を記載しました。ご興味のある方は触ってみてください）。\n",
        " - urllib.request: 指定されたURLを開きHTTPレスポンスオブジェクトを取得するためのモジュール。pythonにおけるリクエストとレスポンスのデータハンドリングでは基本的な機能\n",
        " - request: `urllib.request`と同等の機能を有するモジュール。`urllib`と異なり、GETリスクエストをgetメソッドで実行可能など、直感的な操作が可能である\n",
        " - Selenium: Webブラウザを自動的に操作するフレームワーク。ブラウザ操作が可能なのでJSによって動的にコンテンツが生成されるwebからのデータ取得などで利用する\n",
        " - Scrapy: 大規模なWebコンテンツのクローリングプロジェクトに適したフレームワーク。効率的なデータ抽出、データ処理、データ保存の機能を提供\n"
      ],
      "metadata": {
        "id": "ArwFef72CRYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### 非同期でデータを取得するメリット\n",
        "非同期のメリットは、時間がかかる処理で全体の処理が止まらない点です。<br>先述の方法でも困らないとき、非同期処理を利用する必要はありません。web上にある特定のデータを収集するのみ、などでしたら積極的に非同期処理を使用することは少ないと考えられます。<br>一方で、データを取得した後に続けて処理が行われるときは、同期処理ではユーザー応答に時間がかかる可能性が生じてきます（例えば、後述のSeleniumによるデータ取得を実行していただくとかなり時間がかかることを体感できます）。<br>urlへアクセスしデータを取得する一連のプロセスを非同期処理として分離することで応答速度を向上させることができます。\n",
        "\n",
        " ### 非同期処理でwebへアクセスする方法\n",
        "非同期的な処理で指定したurlよりデータを取得する方法もいくつかあります。有名なものは以下の通りでしょうか\n",
        " - `asyncio + aiohttp`: 非同期処理の基本的な構成で高いパフォーマンスを発揮する\n",
        " - `asyncio + httpx`: 高いパフォーマンスを発揮する。またhttpxはrequestsのAPIと互換性があるため同期処理からの移行時に便利\n",
        " - `Grequests`: 同様にrequestsのAPIと互換性のある、requestsライブラリをベースとした非同期HTTPリクエストライブラリ。グリーンスレッド（OSと異なる仮想的な空間で動作するスレッド）を利用した軽量な処理が可能。ノンブロッキングI/O（I/O処理が完了するまでプログラムの実行が停止しない仕組み）が可能\n",
        " - `Twisted`: 非同期ネットワークプログラミングのためのフレームワーク。歴史が長く、安定しており、幅広いプロトコル（TCP, UDP, SSL/TLSなど）をサポート。チャットサーバーやゲームサーバーなど、多様なアプリケーションの開発に用いられている\n",
        "\n",
        "## `asyncio + aiohttp`の実装\n",
        "今回は一例として`asyncio + aiohttp`で非同期処理を実装してみたいと思います。（気が向いたら他の方法での実装もやってみます）<br>[Qiitaサイト](https://qiita.com/)、[Zennサイト](https://zenn.dev/)アクセスしホーム画面にpostされている記事を取得する処理を実装してみます。非同期処理を使用せずに処理行った時との比較は後述の\"同期処理 実装例（おまけ）\"を触ってみてください。簡単にコードの説明も記載しておきます。\n",
        " - `aiohttp.ClientSession() as session`: *HTTPクライアントとしての機能を提供するオブジェクト。Session情報を管理する\n",
        " - `asyncio.gather()`: 引数に渡されたタスクを並列処理するメソッド\n",
        "\n",
        " ※ HTTPクライアントとは、HTTP (Hypertext Transfer Protocol) を使用して、HTTPサーバーにリクエストを送信し、レスポンスを受信するソフトウェアまたはライブラリのこと"
      ],
      "metadata": {
        "id": "vttB255nQNjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "\n",
        "# 非同期で対象urlのhtmlデータを取得する\n",
        "async def fetch_data(url):\n",
        "    # Session情報を管理するオブジェクトを作成\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        # GETリクエストでresponseを取得\n",
        "        async with session.get(url) as response:\n",
        "            # textメソッドで取得したresponseのContent-Typeを読み取り自動でデコードを行う\n",
        "            return await response.text()\n",
        "\n",
        "async def main():\n",
        "    # 取得したHTMLからaタグのurlをパースするための正規表現（htmlの中身によって調整は必要）\n",
        "    qiita_url_match = re.compile(r\"(?<=a href\\=\\\")https://qiita.com/.*?(?=\\\")\") # Qiita\n",
        "    zenn_url_match = re.compile(r\"(?<=href\\=\\\")[^>]*articles[^>]*?(?=\\\")\") # Zenn\n",
        "    # 各記事のtitleタグをパースするための正規表現\n",
        "    title_match = re.compile(r\"(?<=\\<title\\>).*?(?=\\<\\/title\\>)\")\n",
        "    # 取得先のurlをリストで指定\n",
        "    urls = [\n",
        "        r\"https://qiita.com/\",\n",
        "        r\"https://zenn.dev/\"\n",
        "    ]\n",
        "\n",
        "    # 非同期処理用のタスクを作成し、イベントループに登録\n",
        "    tasks = [fetch_data(url) for url in urls]\n",
        "    # イベントループ内のタスクを並列処理で実行\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    # 取得したurlを格納する箱\n",
        "    qiita_match_urls = []\n",
        "    zenn_match_urls = []\n",
        "    # 各タスクの結果からresultを取得して処理\n",
        "    for result in results:\n",
        "        # print(result) # 結果確認\n",
        "        url_match = qiita_url_match if \"qiita\" in result else zenn_url_match\n",
        "        # 文字列を一行ごとに分割\n",
        "        html_lines = result.splitlines()\n",
        "        for line in html_lines:\n",
        "          if \"qiita\" in result:\n",
        "            qiita_match_urls += url_match.findall(line)\n",
        "          else:\n",
        "            zenn_match_urls += url_match.findall(line)\n",
        "    # 集合を用いて重複削除（画面に描画されている記事を上から5つ分）\n",
        "    qiita_match_urls = list(sorted(set(qiita_match_urls), key=qiita_match_urls.index))[:5]\n",
        "    zenn_match_urls = [str(r\"https://zenn.dev\" + url) for url in list(sorted(set(zenn_match_urls), key=zenn_match_urls.index))][:5]\n",
        "\n",
        "    # Postされている記事のurlを再度格納\n",
        "    urls = qiita_match_urls + zenn_match_urls\n",
        "    # 非同期処理用のタスクを作成し、イベントループに登録\n",
        "    tasks = [fetch_data(url) for url in urls]\n",
        "    article_titles = await asyncio.gather(*tasks)\n",
        "    # print(article_titles) # 結果確認\n",
        "    # 各記事のタイトルを取得するため各urlのtitleを抽出（上位5つ)\n",
        "    for i, url in enumerate(urls):\n",
        "      print(title_match.findall(article_titles[i])[0], \": \", url)\n",
        "\n",
        "# 実行\n",
        "if __name__ == \"__main__\":\n",
        "  # jupyter notebook上では、asyncio.runでは動かないためawaitで実行\n",
        "  await main()"
      ],
      "metadata": {
        "id": "IdHWOlfMXPv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 同期処理 実装例（おまけ）"
      ],
      "metadata": {
        "id": "LS2GcDWCAPO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "※`urllib.request`によるデータ取得例"
      ],
      "metadata": {
        "id": "JhjqM20FWH_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knVK51_zCPgE"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import re\n",
        "\n",
        "'''qiita, zennにpostされている記事のリンクを取得する'''\n",
        "def fetch_data(url: str, pattern: re):\n",
        "  try:\n",
        "      # requestによって取得したHTTPレスポンスを\"response\"として保持\n",
        "      with urllib.request.urlopen(url) as response:\n",
        "          # read()でhtmlを読み込む\n",
        "          html = response.read()\n",
        "          # 取得したデータ(bytes型)をデコードして文字列にする\n",
        "          html_str = html.decode(\"utf-8\")  # または \"shift-jis\" など、適切なエンコーディングを指定\n",
        "          # print(html_str) # 結果確認\n",
        "          # 文字列を一行ごとに分割\n",
        "          html_lines = html_str.splitlines()\n",
        "          # HTML一行ごとに情報を抜いてくる\n",
        "          match_items = []\n",
        "          for line in html_lines:\n",
        "            match_items += pattern.findall(line)\n",
        "            if not(match_items):\n",
        "              continue\n",
        "          # 集合を用いて重複削除（画面に描画されている記事を上から5つ分）\n",
        "          # zennの場合はhrefで抽出したurlに加筆\n",
        "          if url == r\"https://zenn.dev/\":\n",
        "            match_items = [str(r\"https://zenn.dev\" + item) for item in match_items]\n",
        "          return list(sorted(set(match_items), key=match_items.index))[:5]\n",
        "\n",
        "  except urllib.error.URLError as e:\n",
        "      print(f\"URLエラー: {e.reason}\")\n",
        "  except urllib.error.HTTPError as e:\n",
        "      print(f\"HTTPエラー: {e.code}\")\n",
        "  except Exception as e:\n",
        "      print(f\"エラーが発生しました: {e}\")\n",
        "\n",
        "# 取得先のurlをリストで指定\n",
        "urls = [\n",
        "    r\"https://qiita.com/\",\n",
        "    r\"https://zenn.dev/\"\n",
        "]\n",
        "# 取得したHTMLからaタグのurlをパースするための正規表現（htmlの中身によって調整は必要）\n",
        "qiita_url_match = re.compile(r\"(?<=a href\\=\\\")https://qiita.com/.*?(?=\\\")\") # Qiita\n",
        "zenn_url_match = re.compile(r\"(?<=href\\=\\\")[^>]*articles[^>]*?(?=\\\")\") # Zenn\n",
        "# 各記事のtitleタグをパースするための正規表現\n",
        "title_match = re.compile(r\"(?<=\\<title\\>).*?(?=\\<\\/title\\>)\")\n",
        "\n",
        "\n",
        "# 実行\n",
        "# 取得した記事urlのリスト\n",
        "article_urls = []\n",
        "for url, match_pattern in zip(urls, [qiita_url_match, zenn_url_match]):\n",
        "  article_urls += fetch_data(url, match_pattern)\n",
        "\n",
        "## 各記事のタイトルを取得するため各urlのtitleを抽出（上位5つずつ）\n",
        "for url in article_urls:\n",
        "  article_title = fetch_data(url, title_match)\n",
        "  print(article_title[0], \": \", url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "※`request`によるデータ取得例"
      ],
      "metadata": {
        "id": "imOKC4ryM_B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "'''qiita, zennにpostされている記事のリンクを取得する'''\n",
        "def fetch_data(url: str, pattern: re):\n",
        "  try:\n",
        "    # GETリクエストでレスポンスを取得。\n",
        "    response = requests.get(url, timeout=10) # 10秒でタイムアウト\n",
        "    # HTTPエラーが発生した場合に例外を発生させる処理\n",
        "    response.raise_for_status()\n",
        "    # responseのbodyを文字列として取得 (response.textでは自動でデコードする)\n",
        "    # requestsではレスポンスヘッダーのContent-Typeに含まれるcharset情報に基づいて自動的にエンコーディングを判別しデコードする\n",
        "    # 文字コードはencodeingプロパティが保持しており、明示的に指示する時にはresponse.encoding = \"XXXXXX\" と指定する\n",
        "    html = response.text\n",
        "    # print(html) # 結果確認\n",
        "    # 文字列を一行ごとに分割\n",
        "    html_lines = html.splitlines()\n",
        "    # HTML一行ごとに情報を抜いてくる\n",
        "    match_items = []\n",
        "    for line in html_lines:\n",
        "      match_items += pattern.findall(line)\n",
        "      if not(match_items):\n",
        "        continue\n",
        "    # 集合を用いて重複削除（画面に描画されている記事を上から5つ分）\n",
        "    # zennの場合はhrefで抽出したurlに加筆\n",
        "    if url == r\"https://zenn.dev/\":\n",
        "      match_items = [str(r\"https://zenn.dev\" + item) for item in match_items]\n",
        "    return list(sorted(set(match_items), key=match_items.index))[:5]\n",
        "\n",
        "  except requests.exceptions.Timeout:\n",
        "    print(\"タイムアウトしました\")\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"エラーが発生しました: {e}\")\n",
        "\n",
        "# 取得先のurlをリストで指定\n",
        "urls = [\n",
        "    r\"https://qiita.com/\",\n",
        "    r\"https://zenn.dev/\"\n",
        "]\n",
        "# 取得したHTMLからaタグのurlをパースするための正規表現（htmlの中身によって調整は必要）\n",
        "qiita_url_match = re.compile(r\"(?<=a href\\=\\\")https://qiita.com/.*?(?=\\\")\") # Qiita\n",
        "zenn_url_match = re.compile(r\"(?<=href\\=\\\")[^>]*articles[^>]*?(?=\\\")\") # Zenn\n",
        "# 各記事のtitleタグをパースするための正規表現\n",
        "title_match = re.compile(r\"(?<=\\<title\\>).*?(?=\\<\\/title\\>)\")\n",
        "\n",
        "# 実行\n",
        "# 取得した記事urlのリスト\n",
        "article_urls = []\n",
        "for url, match_pattern in zip(urls, [qiita_url_match, zenn_url_match]):\n",
        "  article_urls += fetch_data(url, match_pattern)\n",
        "\n",
        "## 各記事のタイトルを取得するため各urlのtitleを抽出（上位5つずつ）\n",
        "for url in article_urls:\n",
        "  article_title = fetch_data(url, title_match)\n",
        "  print(article_title[0], \": \", url)"
      ],
      "metadata": {
        "id": "DrXgah_bAeor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "※`selenium`によるデータ取得例（`selenium`のinstallを忘れずに実行してください）"
      ],
      "metadata": {
        "id": "3b5ocTnOCqFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install selenium # 初回ではseleniumをinstallする\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import re\n",
        "\n",
        "'''qiita, zennにpostされている記事のリンクを取得する'''\n",
        "def fetch_data(url: str, pattern: re):\n",
        "  # Headless Chromeの設定\n",
        "  # 起動時の設定を管理するoptionsオブジェクトを作成\n",
        "  options = webdriver.ChromeOptions()\n",
        "  # ヘッドレスモードで動かすように設定。ヘッドレスモードはGUIなしで動くためバックグラウンドで動かせる\n",
        "  options.add_argument(\"--headless\")\n",
        "  # GUIを使用しないので念の為GPUのレンダリングを無効に設定。\n",
        "  options.add_argument(\"--disable-gpu\")\n",
        "  # sandbox環境での実行を無効に設定。sandbox環境はブラウザを仮想的な環境で実行する機能。有効にすることでセキュリティが向上するがColab上ではエラーとなるため無効\n",
        "  options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "  try:\n",
        "    # Chromeを起動(optionsは上記の設定を反映するため)\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    # driverオブジェクトのget()メソッドを用いてChrome上でGETリクエストを出す。\n",
        "    driver.get(url)\n",
        "    # アクセス先のHTMLを取得するためpage_sourceを取得\n",
        "    html = driver.page_source\n",
        "    # print(html) 確認用\n",
        "    html_lines = html.splitlines()\n",
        "    # HTML一行ごとに情報を抜いてくる\n",
        "    match_items = []\n",
        "    for line in html_lines:\n",
        "      match_items += pattern.findall(line)\n",
        "      if not(match_items):\n",
        "        continue\n",
        "    # 集合を用いて重複削除（画面に描画されている記事を上から5つ分）\n",
        "    # zennの場合はhrefで抽出したurlに加筆\n",
        "    if url == r\"https://zenn.dev/\":\n",
        "      match_items = [str(r\"https://zenn.dev\" + item) for item in match_items]\n",
        "    return list(sorted(set(match_items), key=match_items.index))[:5]\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"エラーが発生しました: {e}\")\n",
        "\n",
        "\n",
        "# 取得先のurlをリストで指定\n",
        "urls = [\n",
        "    r\"https://qiita.com/\",\n",
        "    r\"https://zenn.dev/\"\n",
        "]\n",
        "# 取得したHTMLからaタグのurlをパースするための正規表現（htmlの中身によって調整は必要）\n",
        "qiita_url_match = re.compile(r\"(?<=a href\\=\\\")https://qiita.com/.*?(?=\\\")\") # Qiita\n",
        "zenn_url_match = re.compile(r\"(?<=href\\=\\\")[^>]*articles[^>\\.js]*?(?=\\\")\") # Zenn\n",
        "# 各記事のtitleタグをパースするための正規表現\n",
        "title_match = re.compile(r\"(?<=\\<title\\>).*?(?=\\<\\/title\\>)\")\n",
        "\n",
        "# 実行\n",
        "# 取得した記事urlのリスト\n",
        "article_urls = []\n",
        "for url, match_pattern in zip(urls, [qiita_url_match, zenn_url_match]):\n",
        "  article_urls += fetch_data(url, match_pattern)\n",
        "\n",
        "## 各記事のタイトルを取得するため各urlのtitleを抽出（上位5つずつ）\n",
        "for url in article_urls:\n",
        "  article_title = fetch_data(url, title_match)\n",
        "  print(article_title[0], \": \", url)\n"
      ],
      "metadata": {
        "id": "p4Pcwl4kC3lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}